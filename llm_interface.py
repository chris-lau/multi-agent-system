"""
LLM Integration for Multi-Agent Research System
Uses Google's Gemini LLM for research tasks
"""
import google.generativeai as genai
import os
from typing import Dict, Any, List
import json
from urllib.parse import quote


class GeminiLLMInterface:
    """Interface to interact with Google's Gemini LLM for research tasks"""
    
    def __init__(self):
        # Initialize the Gemini API with the API key
        api_key = os.environ.get('GOOGLE_API_KEY')
        model_name = os.environ.get('GEMINI_MODEL', 'gemini-pro')
        
        if not api_key:
            # For demo purposes, we'll show what would be needed
            print("Note: To use real Gemini LLM, set your GOOGLE_API_KEY environment variable.")
            print("For demo purposes, using mock responses...")
            self.use_mock = True
        else:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(model_name)
            self.use_mock = False
    
    def perform_technical_research(self, query: str, context: str) -> Dict[str, Any]:
        """Use Gemini LLM to perform technical research on the query"""
        if self.use_mock:
            # Return a mock response for demonstration
            return {
                "findings": f"Technical analysis of '{query}': This involves advanced computing methodologies and requires specific technical infrastructure.",
                "sources": ["Tech Database A", "Technical Journal B", "Patent Database C"],
                "confidence": 0.85,
                "timestamp": "2023-10-01T10:00:00Z"
            }
        
        # Create the prompt for technical research
        prompt = f"""
        As a technical research expert, analyze the technical aspects of the following query: {query}
        Context: {context}
        
        Provide your findings in the following JSON format:
        {{
          "findings": "detailed technical analysis",
          "sources": ["source1", "source2", "source3"],
          "confidence": 0.0-1.0
        }}
        
        Be specific and provide factual information based on current technology trends and capabilities.
        """
        
        try:
            response = self.model.generate_content(prompt)
            # Extract the JSON from the response
            text_response = response.text.strip()
            
            # Try to find JSON in the response
            start_idx = text_response.find('{')
            end_idx = text_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = text_response[start_idx:end_idx]
                result = json.loads(json_str)
                result["timestamp"] = "2023-10-01T10:00:00Z"  # Add timestamp
                return result
            else:
                # If no JSON found, return a default structure
                return {
                    "findings": response.text,
                    "sources": ["Generated by Gemini LLM"],
                    "confidence": 0.7,
                    "timestamp": "2023-10-01T10:00:00Z"
                }
        except Exception as e:
            print(f"Error in technical research: {e}")
            return {
                "findings": f"Error in technical analysis: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "timestamp": "2023-10-01T10:00:00Z"
            }
    
    def perform_economic_research(self, query: str, context: str) -> Dict[str, Any]:
        """Use Gemini LLM to perform economic research on the query"""
        if self.use_mock:
            # Return a mock response for demonstration
            return {
                "findings": f"Economic implications of '{query}': This would require an investment of approximately $X million with an estimated ROI of Y% over Z years.",
                "sources": ["Economic Database A", "Financial Journal B", "Market Analysis C"],
                "confidence": 0.78,
                "timestamp": "2023-10-01T10:00:00Z"
            }
        
        # Create the prompt for economic research
        prompt = f"""
        As an economic research expert, analyze the economic implications of the following query: {query}
        Context: {context}
        
        Provide your findings in the following JSON format:
        {{
          "findings": "detailed economic analysis",
          "sources": ["source1", "source2", "source3"],
          "confidence": 0.0-1.0
        }}
        
        Include information about costs, benefits, market impacts, and economic trends related to the query.
        """
        
        try:
            response = self.model.generate_content(prompt)
            text_response = response.text.strip()
            
            # Try to find JSON in the response
            start_idx = text_response.find('{')
            end_idx = text_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = text_response[start_idx:end_idx]
                result = json.loads(json_str)
                result["timestamp"] = "2023-10-01T10:00:00Z"  # Add timestamp
                return result
            else:
                # If no JSON found, return a default structure
                return {
                    "findings": response.text,
                    "sources": ["Generated by Gemini LLM"],
                    "confidence": 0.7,
                    "timestamp": "2023-10-01T10:00:00Z"
                }
        except Exception as e:
            print(f"Error in economic research: {e}")
            return {
                "findings": f"Error in economic analysis: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "timestamp": "2023-10-01T10:00:00Z"
            }
    
    def perform_fact_checking(self, research_results: Dict[str, Any]) -> Dict[str, Any]:
        """Use Gemini LLM to fact-check research results"""
        if self.use_mock:
            # Return mock validation results for demonstration
            validation_results = {}
            for agent_type, result in research_results.items():
                confidence = result.get("confidence", 0.5)
                validation_status = "verified" if confidence > 0.7 else "partially verified"
                
                validation_results[agent_type] = {
                    "status": validation_status,
                    "confidence": confidence,
                    "sources_checked": result.get("sources", []),
                    "timestamp": "2023-10-01T11:00:00Z"
                }
            return validation_results
        
        # Create the prompt for fact-checking
        results_str = json.dumps(research_results, indent=2)
        prompt = f"""
        As a fact-checking expert, validate the following research results:
        {results_str}
        
        For each research result, verify the accuracy of the information and provide validation results in the following JSON format:
        {{
          "tech": {{
            "status": "verified|partially verified|unverified|incorrect",
            "confidence": 0.0-1.0,
            "sources_checked": ["source1", "source2"],
            "issues": ["list of any issues found"]
          }},
          "economic": {{
            "status": "verified|partially verified|unverified|incorrect",
            "confidence": 0.0-1.0,
            "sources_checked": ["source1", "source2"],
            "issues": ["list of any issues found"]
          }}
        }}
        
        Be thorough in your verification and note any inconsistencies or potential inaccuracies.
        """
        
        try:
            response = self.model.generate_content(prompt)
            text_response = response.text.strip()
            
            # Try to find JSON in the response
            start_idx = text_response.find('{')
            end_idx = text_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = text_response[start_idx:end_idx]
                result = json.loads(json_str)
                # Add timestamp to each result
                for agent_type in result:
                    result[agent_type]["timestamp"] = "2023-10-01T11:00:00Z"
                return result
            else:
                # If no JSON found, return a default structure
                return {
                    "validation_response": response.text,
                    "timestamp": "2023-10-01T11:00:00Z"
                }
        except Exception as e:
            print(f"Error in fact-checking: {e}")
            # Return validation results indicating error
            validation_results = {}
            for agent_type in research_results.keys():
                validation_results[agent_type] = {
                    "status": "error",
                    "confidence": 0.0,
                    "sources_checked": [],
                    "issues": [f"Fact-checking error: {str(e)}"],
                    "timestamp": "2023-10-01T11:00:00Z"
                }
            return validation_results
    
    def perform_social_cultural_research(self, query: str, context: str) -> Dict[str, Any]:
        """Use Gemini LLM to perform social and cultural research on the query"""
        if self.use_mock:
            # Return a mock response for demonstration
            return {
                "findings": f"Social and cultural implications of '{query}': This impacts diverse communities differently, with potential effects on social structures, cultural practices, and community dynamics. Considerations include accessibility, representation, and cultural sensitivity.",
                "sources": ["Social Research Database", "Cultural Impact Journal", "Community Analysis Report"],
                "confidence": 0.82,
                "timestamp": "2023-10-01T10:00:00Z"
            }
        
        # Create the prompt for social/cultural research
        prompt = f"""
        As a social and cultural research expert, analyze the social and cultural implications of the following query: {query}
        Context: {context}

        Provide your findings in the following JSON format:
        {{
          "findings": "detailed social and cultural analysis",
          "sources": ["source1", "source2", "source3"],
          "confidence": 0.0-1.0
        }}

        Include information about how this impacts different communities, cultural practices, social structures, and any relevant social justice considerations.
        """
        
        try:
            response = self.model.generate_content(prompt)
            text_response = response.text.strip()
            
            # Try to find JSON in the response
            start_idx = text_response.find('{')
            end_idx = text_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = text_response[start_idx:end_idx]
                result = json.loads(json_str)
                result["timestamp"] = "2023-10-01T10:00:00Z"  # Add timestamp
                return result
            else:
                # If no JSON found, return a default structure
                return {
                    "findings": response.text,
                    "sources": ["Generated by Gemini LLM"],
                    "confidence": 0.7,
                    "timestamp": "2023-10-01T10:00:00Z"
                }
        except Exception as e:
            print(f"Error in social/cultural research: {e}")
            return {
                "findings": f"Error in social/cultural analysis: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "timestamp": "2023-10-01T10:00:00Z"
            }